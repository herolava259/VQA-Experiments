{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965af9ac-d87c-48bf-a6be-1144c89f81ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c502e-7d50-408c-9e41-5f0ef862f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow>=2.11.0 -q -U\n",
    "%pip install transformers>=4.34.0 -q -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1be222-3b5b-459b-8119-55c14447a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARLLELISM=false\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UseWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eab4cb-e913-4559-bfbe-c8ccfee3e0ce",
   "metadata": {},
   "source": [
    "## Conversational AI with MLflow and DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0137046-d689-45d2-94ce-f1dc9af136f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer \n",
    "import mlflow \n",
    "\n",
    "conversational_pipeline = transformer.pipeline(model=\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"Hi there, chatbot!\",\n",
    "    mlflow.transformers.generate_signature_output(conversational_pipeline, \"Hi there, chatbot!\"),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcd198-dc19-418a-8fb6-00742a476419",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Conversational\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd54c8-3c01-4786-8526-d22a74534a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=conversational_pipeline,\n",
    "        artifact_path=\"chatbot\",\n",
    "        task=\"conversational\",\n",
    "        signature=signature,\n",
    "        input_example=\"A clever and witty question\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b7120-f765-45af-9930-bb2c64fcb947",
   "metadata": {},
   "source": [
    "### 1. Loading and interacting with the Chatbot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc770f23-fb62-4fb7-8741-33220a920d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "first = chatbot.predict(\"What is the best way to get to Atarctica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ab485-c59e-4f28-9868-9a6e2f4341cd",
   "metadata": {},
   "source": [
    "## Deploying a Transformer model as an OpenAI-compatible Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c91e87-01da-41d8-add9-fe8f33ae2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "import mlflow \n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")\n",
    "\n",
    "mlflow.transformers.save_model(\n",
    "    path=\"tinyllama-text-generation\",\n",
    "    transformers_model=generator,\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0e3ed-4fb2-45de-8cbd-b1de09dbdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= mlflow.pyfunc.load_model(\"tinyllama-text-generation\")\n",
    "\n",
    "model.metadata.signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8cb0b-8602-48e2-95dc-6f931b3226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"user\", \"content\":\"Write me a hello world program by python.\"}]\n",
    "\n",
    "prompt = generator.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78033cf4-0378-4a8e-9faa-695f02d28d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.transformers.save_model(\n",
    "    path=\"tinyllama-chat\",\n",
    "    transformer_model=generator,\n",
    "    task=\"llm/v1/chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad207ec-d630-47e7-8fb6-7b29b0e2f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(\"tinyllama-chat\")\n",
    "\n",
    "model.metadata.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0155a1-aa02-4e57-add4-b2153700434b",
   "metadata": {},
   "source": [
    "## Serving the chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c613d-b432-41f7-91f2-7c48643fa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mlflow models serve -m tinyllama-chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ae00f-676c-439e-9779-8ee013aab0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "curl http://localhost:5000/invocations -H 'Content-Type: application/json' -d '{\"messages\":[{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}]}' | jq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
